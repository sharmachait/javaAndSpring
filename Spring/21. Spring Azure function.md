```java
package com.function;

import com.microsoft.azure.functions.*;
import com.microsoft.azure.functions.annotation.*;
import com.microsoft.azure.storage.*;
import com.microsoft.azure.storage.blob.*;

import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;

import java.io.*;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.*;

public class BlobToParquetFunction {

    private static final String OUTPUT_CONTAINER = "adf-parquet-files";
    private static final String STORAGE_CONNECTION_STRING = "AzureWebJobsStorage";

    @FunctionName("BlobToParquetConverter")
    public void run(
            @BlobTrigger(
                name = "content",
                path = "input-files/{name}",
                connection = "AzureWebJobsStorage"
            ) byte[] content,
            @BindingName("name") String filename,
            final ExecutionContext context
    ) {
        context.getLogger().info("Processing blob: " + filename);
        
        try {
            // Assume CSV format for simplicity - adjust based on your input format
            String fileContent = new String(content, StandardCharsets.UTF_8);
            List<String> lines = Arrays.asList(fileContent.split("\n"));
            
            // Get header and create schema
            String[] headers = lines.get(0).split(",");
            Schema schema = createSchema(headers);
            
            // Create temp file for parquet
            java.nio.file.Path tempFilePath = Files.createTempFile("parquet-", ".parquet");
            File tempFile = tempFilePath.toFile();
            
            // Convert to parquet
            writeToParquet(lines, schema, headers, tempFile.getAbsolutePath(), context);
            
            // Upload to ADF container
            uploadToADFContainer(tempFile, filename.replace(getFileExtension(filename), "parquet"), context);
            
            // Clean up temp file
            tempFile.delete();
            
            context.getLogger().info("Successfully converted and uploaded to ADF container");
            
        } catch (Exception e) {
            context.getLogger().severe("Error processing blob: " + e.getMessage());
            e.printStackTrace();
        }
    }
    
    private Schema createSchema(String[] headers) {
        StringBuilder schemaBuilder = new StringBuilder();
        schemaBuilder.append("{\"type\":\"record\", \"name\":\"DataRecord\", \"fields\":[");
        
        for (int i = 0; i < headers.length; i++) {
            schemaBuilder.append("{\"name\":\"")
                .append(headers[i].trim())
                .append("\", \"type\":[\"null\", \"string\"], \"default\":null}");
                
            if (i < headers.length - 1) {
                schemaBuilder.append(",");
            }
        }
        
        schemaBuilder.append("]}");
        return new Schema.Parser().parse(schemaBuilder.toString());
    }
    
    private void writeToParquet(List<String> lines, Schema schema, String[] headers, String outputPath, ExecutionContext context) throws IOException {
        try (ParquetWriter<GenericRecord> writer = AvroParquetWriter
                .<GenericRecord>builder(new Path(outputPath))
                .withSchema(schema)
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .withConf(new Configuration())
                .build()) {
            
            // Skip header
            for (int i = 1; i < lines.size(); i++) {
                String line = lines.get(i);
                if (line.trim().isEmpty()) continue;
                
                String[] values = line.split(",");
                GenericRecord record = new GenericData.Record(schema);
                
                for (int j = 0; j < headers.length; j++) {
                    String value = j < values.length ? values[j].trim() : null;
                    record.put(headers[j].trim(), value);
                }
                
                writer.write(record);
            }
        } catch (Exception e) {
            context.getLogger().severe("Error writing parquet: " + e.getMessage());
            throw e;
        }
    }
    
    private void uploadToADFContainer(File file, String blobName, ExecutionContext context) throws Exception {
        try {
            // Get storage account from connection string
            CloudStorageAccount storageAccount = CloudStorageAccount.parse(
                System.getenv(STORAGE_CONNECTION_STRING));
            
            // Create the blob client
            CloudBlobClient blobClient = storageAccount.createCloudBlobClient();
            
            // Get reference to the container
            CloudBlobContainer container = blobClient.getContainerReference(OUTPUT_CONTAINER);
            
            // Create the container if it does not exist
            container.createIfNotExists();
            
            // Get reference to the blob
            CloudBlockBlob blob = container.getBlockBlobReference(blobName);
            
            // Upload file
            blob.uploadFromFile(file.getAbsolutePath());
            
            context.getLogger().info("Uploaded parquet file to: " + blob.getUri());
            
        } catch (Exception e) {
            context.getLogger().severe("Error uploading to blob storage: " + e.getMessage());
            throw e;
        }
    }
    
    private String getFileExtension(String filename) {
        if (filename.lastIndexOf(".") != -1 && filename.lastIndexOf(".") != 0) {
            return filename.substring(filename.lastIndexOf("."));
        } else {
            return "";
        }
    }
}
```

