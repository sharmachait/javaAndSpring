also has master slave architecture
master listens to the clients and spins up containers on workers
these managed containers, managed by the master instance is called a pod
we can have multiple containers in a single pod
whats the point of pods?
containers in the same pod can share teh file system
![[Pasted image 20241111234712.png]]

api server is the daemon that listens for kube commands
etcd is a distributed in memory cache
	multiple masters can share data in the etcd
when an api server gets a command to start a pod it puts that info in etcd, basically the end state
then the kube-controller-manager and the kube-scheduler read that information calculate the diff that needs to be perform spins up the pods for us
scheduler runs in an infinite loop and assigns workers to the the pod info stored in the etcd
the controller manager manages
- deployment controller
- jobs controller
- replica set controller
![[Pasted image 20241112000301.png]]

the container runtime is the allocated os resources where the containers run
kubelet is another daemon that keeps on running and spins up containers in the container runtime
kube-proxy is the reverse proxy manager, to redirect http requests to the correct containers

# KIND 

to create kubernetes cluster on your local machine
> curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.18.0/kind-linux-amd64
> chmod +x ./kind
> sudo mv ./kind /usr/local/bin/kind
> kind--version
> kind create cluster --name local

with this we will be able to create a master node on our machine, basically a container named local-control-plane
we can delete this cluster with
> kind delete cluster -n local

# multi node setup in kind
we can create a multi node setup with yml
```yml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes: 
	- role: control-plane
	- role: worker
	- role: worker
```

> kind cluster create --config clusters.yml --name local

now if we check the list of running containers we will see 3 and the control plane will have an api address and a port where we can hit the kubernetes api
why? because the master node has an api server inside it

![[Pasted image 20241122021522.png]]

how to tell the master node to create a pod on a worker node? using the kubernetes API
but it requires authentication
To authenticate its stored in .kube/config
> cat ~/.kube/config

when we use kubectl it tries to find the content of ~/.kube/config
reads the auth creds from there and uses the kubernetes api to send commands to the master node's api server

when we create an aks cluster we they will give us a config file that we need to put in this directory ~./kube from the machine we want to send kubectl commands to the master node

# kubectl
#### we can get all the nodes of the cluster with
> kubectl get nodes

if we want to see the api calls that are going out to the master node we can suffix any command with --v=8
> kubectl get nodes --v=8
![[Pasted image 20241122023122.png]]

#### create a pod 
`kubectl run <podName> --image=<imageName> --port=80`
`kubectl run nginx --image=nginx --port=80`
what this creates looks something like this
![[Pasted image 20241122025603.png]]
its still not enough to expose it over the internet
to discover a pod is a whole different thing

this is one way to create a pod
#### manifest
other way is create a manifest file
```yml
apiVersion: v1
kind: Pod
metadata:
	name: nginx
spec: 
	containers:
		- name: nginx
		  image: nginx
		  ports:
			  - containerPorts: 80
```

`kubectl apply -f manifest.yml`

we can have multiple; pod configs in the same file and when we apply the file both would be applied
![[Pasted image 20241122182137.png]]
or we can create deployments
#### get pods
`kubectl get pods`
`kubectl get pods -w`
this keeps watching we dont have to run the get pods command again and again
#### logs of a pod
`kubectl logs nginx`

#### describe a pod
`kubectl dscribe pod nginx`
![[Pasted image 20241122030435.png]]
#### delete pod
`kubectl delete pod <name>`


if i have multiple container running on the same pod they have the same ip address, because they share same network namespace, this enables to communicate via local host
If two containers in the same Pod try to bind to theÂ **same port**, a conflict will occur because both containers share the same network namespace. The second container will fail to start due to the port collision.
