## Pod with a volume
```yml
apiVersion: v1
kind: Pod
metadata:
  name: random-number
spec:
  volumes:
  - name: data-volume
    hostPath:
      path: /data
      type: Directory
  containers:
  - image: alpine
    name: alpine
    command: ["/bin/sh", "-c"]
    args: ["shuf -i 0-100 -n 1 >> /opt/number.out;"]
    volumeMounts: 
    - mountPath: /opt
      name: data-volume
```
this pod writes a random number in a file called number.out at opt/
it then gets removed along with the file at /opt/number.out
unless we have mounted it at a volume at the host
the simplest way to mount is directory mounting
## volume options
##### 1. Host path
doesnt work in a multi node cluster as data will remain local to one node
##### 2. Cloud provided volume
instead use something like azure or aws disk
```yml
volumes:
- name: data-volume
  azureDisk:
    diskName: <disk-name>
    diskURI: <disk-uri>
    cachingMode: ReadWrite
    fsType: ext4
    kind: Managed
```
##### 3. emptyDir
is a type of volume used to store data in a Pod only as long as that Pod is running

##### 4. configmaps and secrets



all this is very cumbersome, instead we wanna handle storage and volumes centrally, instead of via declaring in each manifest file, thats where Persistent Volumes help us
## Persistent volume and Persistent Volume claim (PVC)
Its a cluster wide pool of storage managed and provisioned by an administrator, and can be used by users deployed application on the cluster, via "claiming" some persistent volume for the Pod
### Creating a persistent volume

Access Modes: defines how a volume should be mounted on the host, whether the applications that claim some piece from this volume will be able to:
1. ReadOnlyMany : Read Only allowed for many pods 
2. ReadWriteOnce : Read and write once to a single pod
3. ReadWriteMany : Read and write allowed for many pods 

hostPath is used to define where each of the nodes in any of the cluster will be able to access this persistent volume
should not be used in production

```yml
apiVersion: v1
kind: PersistentVolume
metadata:
	name: pv-volume1
spec:
	accessModes:
	- ReadWriteOnce
	capacity:
		storage: 1Gi
	hostPath:
		path: /tmp/data  
```

still better to use an azure disk for persistence
```yml
apiVersion: v1
kind: PersistentVolume
metadata:
	name: pv-volume1
spec:
	capacity:
		storage: 1Gi
	accessModes:
	- ReadWriteOnce
	persistentVolumeReclaimPolicy: Retain
	azureDisk:
		diskName: <disk-name>	# Replace with your Azure Disk name
		diskURI: <disk-uri>		# Replace with the full URI of the Azure Disk
		cachingMode: ReadWrite
		fsType: ext4
		kind: Managed			# Managed disk type (recommended for production)
```

>kubectl apply -f manifest.yml
>kubectl get persistentvolume
### claiming a piece from the persistent volume
each claim is bound to a single persistent volume
we can limit how many claims a pod can make and how much amount it can claim

when we claim a PVC then kubernetes auto select which persistent volume it will be bound to
if we want to specify which PV to bind to we can use labels at the PV and selectors at the PVC
if the access mode of the volume doesnt allow multiple pod may not be able to bind to the same volume

if no volume is available the PVC will remain in a pending state untill more PV are not created

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
	name: myclaim
spec:
	accessModes:
	- ReadWriteOnce
	resources:
		requests:
			storage: 500Mi
```

The `accessModes` in the PVC must match the `accessModes` defined in the corresponding PV. This ensures that the volume being claimed supports the requested type of access.

>kubectl get persistentvolumeclaim

### attaching a PVC to a Pod
we can attach a PVC to a pod by defining it as a volume in its spec, same goes for deployments and replicasets
```yml
apiVersion: v1
kind: Pod
metadata:
	name: my-pod
spec:
	volumes:
	  - name: my-volume
	    persistentVolumeClaim:
	        claimName: my-pvc
	containers:
      - name: my-container
        image: nginx
        volumeMounts:
          - mountPath: /usr/share/nginx/html
            name: my-volume
```

By mounting a volume at this path (`mountPath: /usr/share/nginx/html`), the contents of the volume (which comes from the PVC) override the default files in the container's `/usr/share/nginx/html` directory.
- If the PVC contains a file named `index.html`, NGINX will serve this file as the default page.
- If the PVC is empty, NGINX will serve an empty directory at `/usr/share/nginx/html`.

### cleaning up PVC
when we delete a PVC
> kubectl delete persistentvolumeclaim myclaim

the underlying PV is retained
as is the default property defined under
```yml
persistentVolumeReclaimPolicy: Retain
```
and is retained until manually deleted
it can be set to **`Delete`**, that way whenever the claim is deleted the volume will be deleted as well
third option is to **`Recycle`**, that is the data stored in the volume will be deleted but the volume allocation it self will be available for use by new pods
```yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 100Mi
  hostPath:
    path: /pv/log
  persistentVolumeReclaimPolicy: Retain
```

as long as the PVC is being used by  a pod it will be stuck in the terminating state and not actually be deleted

With the `Retain` reclaim policy, the PersistentVolume (PV) is not made available for a new claim after the original PersistentVolumeClaim (PVC) is deleted. Here's what happens:

1. When the PVC is deleted, the PV remains in the cluster
2. The PV's status changes to `Released`
3. The PV is not automatically reusable
4. The PV still contains the previous data

To make the PV available again, an administrator would need to:

1. Manually delete the PV
2. Reconfigure or clean the underlying storage
3. Recreate the PV if needed

storage types like `hostPath` do not genuinely support RWX, despite allowing the configuration, therefor in case of hostpath only one PVC will be able to bind with the PV