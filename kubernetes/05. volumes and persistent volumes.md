## Pod with a volume
```yml
apiVersion: v1
kind: Pod
metadata:
  name: random-number
spec:
  volumes:
  - name: data-volume
    hostPath:
      path: /data
      type: Directory
  containers:
  - image: alpine
    name: alpine
    command: ["/bin/sh", "-c"]
    args: ["shuf -i 0-100 -n 1 >> /opt/number.out;"]
    volumeMounts: 
    - mountPath: /opt
      name: data-volume
```
this pod writes a random number in a file called number.out at opt/
it then gets removed along with the file at /opt/number.out
unless we have mounted it at a volume at the host
the simplest way to mount is directory mounting
## volume options
##### 1. Host path
doesnt work in a multi node cluster as data will remain local to one node
##### 2. Cloud provided volume
instead use something like azure or aws disk
```yml
volumes:
- name: data-volume
  azureDisk:
    diskName: <disk-name>
    diskURI: <disk-uri>
    cachingMode: ReadWrite
    fsType: ext4
    kind: Managed
```


all this is very cumbersome, instead we wanna handle storage and volumes centrally, instead of via declaring in each manifest file, thats where Persistent Volumes help us
## Persistent volume and Persistent Volume claim (PVC)
Its a cluster wide pool of storage managed and provisioned by an administrator, and can be used by users deployed application on the cluster, via "claiming" some persistent volume for the Pod
### Creating a persistent volume

Access Modes: defines how a volume should be mounted on the host, whether the applications that claim some piece from this volume will be able to:
1. ReadOnlyMany : Read Only allowed for many pods 
2. ReadWriteOnce : Read and write once to a single pod
3. ReadWriteMany : Read and write allowed for many pods 

hostPath is used to define where each of the nodes in any of the cluster will be able to access this persistent volume
should not be used in production

```yml
apiVersion: v1
kind: PersistentVolume
metadata:
	name: pv-volume1
spec:
	accessModes:
	- ReadWriteOnce
	capacity:
		storage: 1Gi
	hostPath:
		path: /tmp/data  
```

still better to use an azure disk for persistence
```yml
apiVersion: v1
kind: PersistentVolume
metadata:
	name: pv-volume1
spec:
	capacity:
		storage: 1Gi
	accessModes:
	- ReadWriteOnce
	persistentVolumeReclaimPolicy: Retain
	azureDisk:
		diskName: <disk-name>	# Replace with your Azure Disk name
		diskURI: <disk-uri>		# Replace with the full URI of the Azure Disk
		cachingMode: ReadWrite
		fsType: ext4
		kind: Managed			# Managed disk type (recommended for production)
```

>kubectl apply -f manifest.yml
>kubectl get persistentvolume
### claiming a piece from the persistent volume
each claim is bound to a single persistent volume
we can limit how many claims a pod can make and how much amount it can claim

when we claim a PVC then kubernetes auto select which persistent volume it will be bound to
if we want to specify which PV to bind to we can use labels at the PV and selectors at the PVC
if the access mode of the volume doesnt allow multiple pod may not be able to bind to the same volume

if no volume is available the PVC will remain in a pending state untill more PV are not created

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
	name: myclaim
spec:
	accessModes:
	- ReadWriteOnce
	resources:
		requests:
			storage: 500Mi
```

The `accessModes` in the PVC must match the `accessModes` defined in the corresponding PV. This ensures that the volume being claimed supports the requested type of access.

>kubectl get persistentvolumeclaim

