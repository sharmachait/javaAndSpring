## Pod with a volume
```yml
apiVersion: v1
kind: Pod
metadata:
  name: random-number
spec:
  volumes:
  - name: data-volume
    hostPath:
      path: /data
      type: Directory
  containers:
  - image: alpine
    name: alpine
    command: ["/bin/sh", "-c"]
    args: ["shuf -i 0-100 -n 1 >> /opt/number.out;"]
    volumeMounts: 
    - mountPath: /opt
      name: data-volume
```
this pod writes a random number in a file called number.out at opt/
it then gets removed along with the file at /opt/number.out
unless we have mounted it at a volume at the host
the simplest way to mount is directory mounting
## volume options
##### 1. Host path
doesnt work in a multi node cluster as data will remain local to one node
##### 2. Cloud provided volume
instead use something like azure or aws disk
```yml
volumes:
- name: data-volume
  azureDisk:
    diskName: <disk-name>
    diskURI: <disk-uri>
    cachingMode: ReadWrite
    fsType: ext4
    kind: Managed
```
##### 3. emptyDir
is a type of volume used to store data in a Pod only as long as that Pod is running
primarily used to make containers on the same pod share some data
##### 4. configmaps and secrets

all this is very cumbersome, instead we wanna handle storage and volumes centrally, instead of via declaring in each manifest file, thats where Persistent Volumes help us
## Persistent volume and Persistent Volume claim (PVC)
Its a cluster wide pool of storage managed and provisioned by an administrator, and can be used by users deployed application on the cluster, via "claiming" some persistent volume for the Pod
can be provisioned statically or dynamically

### Creating a persistent volume

Access Modes: defines how a volume should be mounted on the host, whether the applications that claim some piece from this volume will be able to:
1. ReadOnlyMany : Read Only allowed for many pods 
2. ReadWriteOnce : Read and write once to a single pod
3. ReadWriteMany : Read and write allowed for many pods 

hostPath is used to define where each of the nodes in any of the cluster will be able to access this persistent volume
should not be used in production

```yml
apiVersion: v1
kind: PersistentVolume
metadata:
	name: pv-volume1
spec:
	accessModes:
	- ReadWriteOnce
	capacity:
		storage: 1Gi
	hostPath:
		path: /tmp/data  
```

still better to use an azure disk for persistence
```yml
apiVersion: v1
kind: PersistentVolume
metadata:
	name: pv-volume1
spec:
	capacity:
		storage: 1Gi
	accessModes:
	- ReadWriteOnce
	persistentVolumeReclaimPolicy: Retain
	azureDisk:
		diskName: <disk-name>	# Replace with your Azure Disk name
		diskURI: <disk-uri>		# Replace with the full URI of the Azure Disk
		cachingMode: ReadWrite
		fsType: ext4
		kind: Managed			# Managed disk type (recommended for production)
```

>kubectl apply -f manifest.yml
>kubectl get persistentvolume

we can also use the networked file system running on aws or azure or on prem
we can create VM on azure or aws, run a container of an NFS and mount it to the host
we can use this VM with nfs running as persistent volume and then our pods can use claims to store data in the VM
creating a persistent volume of type nfs
```yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity: 
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  storageClassName: nfs
  nfs:
    path: /exports
    server: 52.66.197.168
```
the storageClassName is like the selector and label for PV and PVCs
only PVCs with storageClassName: nfs can bind to this volume
it also helps create PV dynamically
```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: nfs-pvc
spec: 
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: nfs
```
and we can use this PVC in a pod as we would regularly
```yml
apiVersion: v1
kind: Pod
metadata:
  name: mongo-pod
spec:
  volumes:
  - name: nfs-volume
    persistentVolumeClaim:
      claimName: nfs-pvc
  containers:
  - name: mongo
    image: mongo
    ports:
    - containerPort: 27017
    volumeMounts:
    - mountPath: "/data/db"
      name: nfs-volume
```
### Automatic Persistent value creation
when a `PersistentVolumeClaim` (PVC) specifies `storageClassName: nfs`, Kubernetes will use the associated **NFS provisioner** (configured in the `StorageClass` named `nfs`) to automatically create a **PersistentVolume (PV)** that satisfies the claim. **(if the provisioner is also deployed to kubernetes)**
we can claim a piece from it with
Every cloud provider supports different storageClassNames 
in case of cloud provided solutions like azure disk before we can create a PV that uses an Azure Disk the Azure Disk must be provisioned
not if we use storage classes, in that case azure will provision the storage resource for us

azure supports the following 4 by default
1. managed-premium(ssd)
2. managed-standard(hdd)
3. azurefile
4. azurefile-premium

so if we are working with azure k8s services, then we can directly start creating PVCs and that will deploy the azure resource for us and create a PV out of it

you need to have the Storage Account already set up and configured with your AKS cluster. AKS comes with the necessary CSI drivers pre-installed.

in case the file share already exists and we just want it mounted to our pods we need to create the PV our selves, and then a PVC out of it
steps:
1. Create a secret with the credentials to access the storage account
```yml
apiVersion: v1
kind: Secret
metadata:
  name: azure-storage-secret
type: Opaque
data:
  azurestorageaccountname: <base64-encoded-storage-account-name>
  azurestorageaccountkey: <base64-encoded-storage-account-key>
```
2. Create a PV
```yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: azurefile-pv
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: azurefile
  azureFile:
    secretName: azure-storage-secret  # contains storage account credentials
    shareName: your-existing-share-name
    readOnly: false
```
3. create a PVC
```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: existing-azurefile-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: azurefile
  resources:
    requests:
      storage: 100Gi  # should match or be less than PV
```

the root folder of the azure files share is mounted

if a Storage Class called  makes use of `VolumeBindingMode` set to `WaitForFirstConsumer`. This will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.
### claiming a piece from the persistent volume
each claim is bound to a single persistent volume
we can limit how many claims a pod can make and how much amount it can claim

when we claim a PVC then kubernetes auto select which persistent volume it will be bound to
if we want to specify which PV to bind to we can use labels at the PV and selectors at the PVC
if the access mode of the volume doesnt allow multiple pod may not be able to bind to the same volume

if no volume is available the PVC will remain in a pending state untill more PV are not created

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
	name: myclaim
spec:
	accessModes:
	- ReadWriteOnce
	resources:
		requests:
			storage: 500Mi
```

The `accessModes` in the PVC must match the `accessModes` defined in the corresponding PV. This ensures that the volume being claimed supports the requested type of access.

>kubectl get persistentvolumeclaim

### attaching a PVC to a Pod
we can attach a PVC to a pod by defining it as a volume in its spec, same goes for deployments and replicasets
```yml
apiVersion: v1
kind: Pod
metadata:
	name: my-pod
spec:
	volumes:
	  - name: my-volume
	    persistentVolumeClaim:
	        claimName: my-pvc
	containers:
      - name: my-container
        image: nginx
        volumeMounts:
          - mountPath: /usr/share/nginx/html
            name: my-volume
```

By mounting a volume at this path (`mountPath: /usr/share/nginx/html`), the contents of the volume (which comes from the PVC) override the default files in the container's `/usr/share/nginx/html` directory.
- If the PVC contains a file named `index.html`, NGINX will serve this file as the default page.
- If the PVC is empty, NGINX will serve an empty directory at `/usr/share/nginx/html`.

we can attach multiple pods to the same PVC
### cleaning up PVC
when we delete a PVC
> kubectl delete persistentvolumeclaim myclaim

the underlying PV is retained
as is the default property defined under
```yml
persistentVolumeReclaimPolicy: Retain
```
and is retained until manually deleted
it can be set to **`Delete`**, that way whenever the claim is deleted the volume will be deleted as well
third option is to **`Recycle`**, that is the data stored in the volume will be deleted but the volume allocation it self will be available for use by new pods
```yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 100Mi
  hostPath:
    path: /pv/log
  persistentVolumeReclaimPolicy: Retain
```

as long as the PVC is being used by  a pod it will be stuck in the terminating state and not actually be deleted

With the `Retain` reclaim policy, the PersistentVolume (PV) is not made available for a new claim after the original PersistentVolumeClaim (PVC) is deleted. Here's what happens:

1. When the PVC is deleted, the PV remains in the cluster
2. The PV's status changes to `Released`
3. The PV is not automatically reusable
4. The PV still contains the previous data

To make the PV available again, an administrator would need to:

1. Manually delete the PV
2. Reconfigure or clean the underlying storage
3. Recreate the PV if needed

storage types like `hostPath` do not genuinely support RWX, despite allowing the configuration, therefor in case of hostpath only one PVC will be able to bind with the PV


### Mounting in Deployments/StatefulSets
all the pods will use the same PVC, that maybe what is desired
but it maybe that we dont want the pods to use the same storage, instead we want each of them to have their own mounts, for example a mysql deployment or redis

can be achieved via **VolumeClaimTemplates**

instead of creating a PVC manually move the spec for the PVC into a property **spec.volumeClaimTemplate** of the stateful set 

```yml
apiVersion: v1
kind: Service
metadata:
  name: mysql-h
spec:
  clusterIP: None
  ports: 
  - port: 3306
  selector:
    app: mysql
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  labels:
    app: mysql
spec: 
  replicas: 3
  serviceName: mysql-h
  selector:
    matchLabels: 
      app: mysql
  template: 
    metadata: 
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql
  volumeClaimTemplates:
  - metadata:
      name: data-volume
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: azurefiles
      resources:
        requests:
          storage: 100Gi  # should match or be less than PV
```

a manifest like this will
1. create the headless service
2. create a statefulset 
3. provisions resource on azure
4. create a PV
5. create a PVC 
6. create a pod
7. do steps 3,4,5,6 for all the replicas
in case a pod goes down it is brought up attached to the already created PVC a new PVC is not created for it again