the kube scheduler is responsible for deciding which node to schedule the pod on
if there is no node with sufficient resources, then it holds back on scheduling the pods
and throws an insufficient cpu or ram error
we can define the resources required for each pod via `resources.requests`

```yml
apiVersion: v1
kind: Pod
metadata:
  name: app
  labels:
    name: app
spec:
  containers: 
  - name: app
    image: app
    ports:
    - containerPort: 8080
    resources:
      requests: 
	    memory: "4Gi"
	    cpu: 2
```

cpu can be defined with floats as well
cpu: 0.1
0.1 is equal to 100m where m stands for milli 
we can go as low as 1m
1 count of CPU is equivalent to 1 vCPU or Azure core

with memory
G = GigaByte ~= Gi
M = MegaByte ~= Mi
K(1000) = KiloByte ~= Ki(1024)

these are the minimum resources that are being requested

containers by default dont have any upper limit on how much resource is gonna be consumed
but that can suffocate the native processes of the node like the kubelet

we can also specify an upper bound to the resources under `resources.limits`

```yml
apiVersion: v1
kind: Pod
metadata:
  name: app
  labels:
    name: app
spec:
  containers: 
  - name: app
    image: app
    ports:
    - containerPort: 8080
    resources:
      requests: 
	    memory: "4Gi"
	    cpu: 2
	  limits:
	    memory: "6Gi"
	    cpu: 3
```

the resources requests and limits need to be defined for each of the containers in case of multi container pods

in case a container tries to go beyond the upper limit the cpu that was allocated to it is throttled to not allow it to go beyond

we dont have such a hard boundary with memory, if a container keeps breaking the limit constantly then an out of memory error is thrown

if we dont specify requests, only limits, both are assumed to be equals for CPU
the issue with defining both is what if there are CPU units to spare and our pods want more CPU, we dont want to limit our pods un necessarily
so ideal scenario could be defining requests and not limits. then each pod is guaranteed CPU and other pods can consume more without suffocating other pods

only limit CPU when absolutely critical
good example of setting limits? leetcode type application, where we provide a sandbox environment to end users, we dont want the users to be able to choke our nodes

for memory if we dont specify requests, limits and requests are assumed to be same here as well, essentially everything is same for memory except if there is no memory left and pod 2 needs more, and pod 1 is hogging memory, only way to free up memory is to kill pod 1

its cumbersome to define resources and limits for each pod 

its better to define them globally(at the namespace level) as defaults, can be done via LimitRanges

```yml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - default:
      cpu: 500m
    defaultRequest:
      cpu: 500m
    max:
      cpu: "1"
    min: 
      cpu: 100m
    type: Container
---
apiVersion: v1
kind: LimitRange
metadata:
  name: memory-resource-constraint
spec:
  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min: 
      memory: 500Mi
    type: Container
```

these ranges are applied at the time of creation of pod, so if i change a limit, already existing pods will not change

## quotas
if we want the sum total of all the Pods resources to be limited as a collective, we can do that via quotas at a namespace level

```yml
apiVersion: v1
kind: ResourceQuotas
metadata:
  name: resource-quota
spec:
  hard: 
    requests.cpu: 4
    requests.memory: 4Gi
    limits.cpu: 10
    limits.memory: 10Gi
```