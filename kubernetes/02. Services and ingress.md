If we ssh into a node we can ping our pods, via the ip address that is allocated to the pod

## Service (exposing pods)

since pods are ephemeral they die and come back up, and with new IP addresses, each pod has a unique IP address
its hard to keep track of them
that where services come in

a service is what exposes our pods to internet, services have an ip address and a dns record 

we can see the IP address of our pods with
> kubectl get pods -owide

these are private IPs and can be used by the pods to talk to each other
but not through the internet

![[Pasted image 20241127000045.png]]

an abstraction that logically define the pods and the set of policies by which to access them. 
allows us to expose application in pods as network services

Services use labels to select the pods that are being targeted

there are three types of services
1. **ClusterIP** - exposes the service on an internal IP of the cluster, this is the default service type. makes the pod accessible only within the cluster, useful for interservice communication
	1. other pods can access them using either the assigned cluster ip or the service name
![[Pasted image 20250108004314.png]]
2. **NodePort** - exposes the service on each Node's IP at a static port. the NodePort routes to a ClusterIP which is automatically created. 
  we can contact the underlying Pod from outside the cluster by requesting `<NodeIp>:<NodePort>`
3. **LoadBalancer** - Exposes the service externally using a cloud providers load balancer. a loadbalancer routes to the NodePorts and ClusterIPs the load balancer routes to are created automatically


## commands to deploy service

creating a ClusterIp
>kubectl  expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

this command will automatically use the pod's labels as selectors
or we can also do this

>kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml

this does not use pod's label as selector instead it assumes **app=redis** as label
its not possible to pass in selectors as an option in this command

```yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: ClusterIP
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
```

##### creating a node port is way  more challenging

> kubectl expose pod nginx --port=80 --name nginx-service --type=NodePort --dry-run=client -o yaml

but this doesnt allow us to specify and change the nodeport, we would have to generate the manifest change the node port to our liking and then re apply, because by default it create a port randomly between 30000-32767

or we can also do
>kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

but like before it doesnt accept a selector so we would have to generate manifest change and apply

### manifest for a service

```yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30007  # This port can be any valid port within the NodePort range
  type: NodePort
```

this will make the pod with selector app: nginx accessible to the outside world on the IP address of the node it is on
if multiple pods have the same labels that match a Service's selector, the Service will automatically load balance traffic between all those pods
the pods can be on different nodes and so we can use the ip address of any node to reach our application via the node port

only for kind - but we dont know the IP of the node, we want to be able to use the IP address of the host machine which we do know
to be able to do that we want to map the nodePort of the node to a port of the host, try to keep them same

```yml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30007
    hostPort: 30007
- role: worker
- role: worker
```

if we start our cluster with this yaml it will map the port 30007 of our localhost to the port 30007 of the nodes
and then that port can be used in NodePort services

but this is only for KIND, this can be done via GUI on azure

now if we have three pods with label app: nginx, the NodePort service will automatically load balance over them

![[Pasted image 20241127005515.png]]
this is only doable in development with KIND
not recommended for production scenarios
this external port mapping on the kind node can be done on any of the nodes not necessarily required to be on the master node

in production we can simply deploy a loadbalancer that will provision an external IP address for us but on some cloud proovider
```yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service-lb
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80         # The port on the LoadBalancer
      targetPort: 80    # The port on the pods
  type: LoadBalancer   # This type triggers the provisioning of an external IP
```
![[Pasted image 20241127020221.png]]
### drawbacks of services

if we have a microservice architecture it will require us to deploy a load balancer for each of the microservice
i will also require multiple certificates for each of the domain names mapped in the load balancers
each load balancer will require us to configure rate limiting in them individually

## Endpoints
endpoints are basically target ip addresses of the pods
a services endpoints can be viewed by describing it
> kubectl describe svc kubernetes
## Ingress

why we need ingresses
if we ever come to the point that we need two load balancer type services deployed on our cluster
we would require a third proxy layer server as well that will redirect instagram.com to LB1 and meta.com to LB2
and it will incur more cost
this also requires managing ssl at different levels
![[Pasted image 20250108012446.png]]
can be solved using ingresses
bur even then we need to expose our ingress to the outside world using a node port or a loadbalancer
if we want to use ingresses we have to install the ingress controller
popular options are nginx-ingress-controller and haproxy-ingress-controller

ingress is an api object to manage external access to services in a cluster via http.
load balacing
ssl termination
name based virtual hosting
![[Pasted image 20241128081003.png]]
![[Pasted image 20241128102505.png]]
1. create deployment
2. create cluster ip service
3. create ingress - will create the load balancer for us

### installing ingress controller

nginx-ingress-controller.yml created by helm

helm helps manage these charts
>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
>helm repo update
>helm install nginx-ingress ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace

or we can write the manifest to deploy the ingress-controller
1. deployment
2. configmap
3. service
4. serviceaccount
and then define the ingress resource with the config
```yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      ports:
      - name: http
        containerPort: 80
      - name: https
        containerPort: 443
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            fieldPath: metadata.namespace
      args: 
      - /nginx-ingress-controller
      - --configmap=$(POD_NAMESPACE)/nginx-configuration
      containers:
      - name: nginx-ingress-controller
	        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
```

this images expects some args and env variables

the nginx config options should be passed in as a config map, to run with default values we need to use an empty config map

```yml
kind:ConfigMap
apiVersion: v1
metadata:
  name: ngins-configuration
```

this also needs a nodeport service 

```yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  selector:
    app: nginx-ingress
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
    - port: 443
      targetPort: 443
      protocol: TCP
      name: https
  type: NodePort
```

this will create the pods for the ingress in the ingress-nginx namespace
and a loadbalancer service and a cluster ip/ Node port for the ingress to communicate with other services

the ingress controller needs extra permissions to monitor all the ingress resources and to update the underlying nginx server when some config is changed

it needs a service account for that with the right set of permissions

```yml
apiVersion: v1
kind: ServiceAccunt
metadata:
  name: nginx-ingress-serviceaccount
```

we still need to define the rules as part of the ingress respirce to be able to use the ingress but this creates the infrastructure
we can simply start defining the ingress resource if we apply the ingress helm

![[Pasted image 20241129004654.png]]
to be able to do this we need to use an ingress manifest
nginx.yml
```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: default
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP
```
apache.yml
```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apache-deployment
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: apache
  template:
    metadata:
      labels:
        app: apache
    spec:
      containers:
      - name: my-apache-site
        image: httpd:2.4
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: apache-service
  namespace: default
spec:
  selector:
    app: apache
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP
```
ingress.yml
```yml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-apps-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: your-domain.com
    http:
      paths:
      - path: /nginx
        pathType: Prefix
        backend:
          service:
            name: nginx-service
            port:
              number: 80
      - path: /apache
        pathType: Prefix
        backend:
          service:
            name: apache-service
            port:
              number: 80
```
if we dont specify the host name, then it considers host to be ** and will match anything

the annotation
```yml
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
```

defines that any request that comes maybe like "/apache" should be redirected to "/" when it reaches the pod serving this request, that is the landing page of the service
maybe because our code doesnt work on /apache and /nginx

we can also do global ratelimiting with ingresses through annotations
```yml
  annotations: 
	nginx.ingress.kubernetes.io/rps: 10
```

### imperative commands to create ingresses

> **`kubectl create ingress <ingress-name> --rule="host/path=service:port"`**

example
> kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"

to find the default backend of an ingress check its controller
> kubectl get deployment name -o yaml
> kubectl get deployment ingress-nginx-controller -n ingress-nginx -o yaml | grep default


if we have a deployment in another namespace then the service for that application and the ingress for that would also be required to be in the same namespace