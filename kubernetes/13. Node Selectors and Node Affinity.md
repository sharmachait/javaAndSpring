not all nodes need to be of the same hardware configuration
node selector can be defined in the pod manifest to tell kubernetes which node it need to run the pod on

```yml
apiVersion: v1
kind: Pod
metadata:
  name: heavy-pod
spec: 
  containers: 
  - name: heavy-container
    image: heavy-image
  nodeSelector:
    size: Large
```

for this to work the node needs to have the `size: Large` label

> `kubectl label nodes <node-name> <label-key>=<label-value>

but selectors have limitations, it hardcodes the node the pod needs to run on and we can define multiple nodes that this may work on
we may have a Large node and a Medium node, there is no way to say that the pod is capable of running on the Large or the Medium node, or on any node that is not Small

to achieve this we need to use node affinity and anti affinity

```yml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: data-processor
    image: data-processor
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - Large
            - Medium
          - key: size
            operator: NotIn
            values:
            - Small
```

in case we have not set the labels for the nodes that are small and we want to schedule only on the nodes that have the label defined with any value we can do that with
```yml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: data-processor
    image: data-processor
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: Exists
```

In case node affinity expression doesnt satisfy any of the nodes? or what if the label is changed after a pod has been scheduled on it?
**requiredDuringSchedulingIgnoredDuringExecution**
in case someone changes the label later it is ignored as pod is in execution
we have other options as well
**preferredDuringSchedulingIgnoredDuringExecution**
with this type of affinity in case there are no nodes that satisfy the affinity rules, the pod is scheduled on any node

**requiredDuringSchedulingRequiredDuringExecution**
this type of affinity is planned for future releases, this will stop the pods in case someone changes the labels