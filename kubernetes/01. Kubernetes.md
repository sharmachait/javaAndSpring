also has master slave architecture
master listens to the clients and spins up containers on workers
these managed containers, managed by the master instance is called a pod
we can have multiple containers in a single pod
whats the point of pods?
containers in the same pod can share teh file system
![[Pasted image 20241111234712.png]]

api server is the daemon that listens for kube commands
etcd is a distributed in memory cache
	multiple masters can share data in the etcd
when an api server gets a command to start a pod it puts that info in etcd, basically the end state
then the kube-controller-manager and the kube-scheduler read that information calculate the diff that needs to be perform spins up the pods for us
scheduler runs in an infinite loop and assigns workers to the the pod info stored in the etcd
the controller manager manages
- deployment controller
- jobs controller
- replica set controller
![[Pasted image 20241112000301.png]]

the container runtime is the allocated os resources where the containers run
kubelet is another daemon that keeps on running and spins up containers in the container runtime
kube-proxy is the reverse proxy manager, to redirect http requests to the correct containers

# KIND 

to create kubernetes cluster on your local machine
> curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.18.0/kind-linux-amd64
> chmod +x ./kind
> sudo mv ./kind /usr/local/bin/kind
> kind--version
> kind create cluster --name local

with this we will be able to create a master node on our machine, basically a container named local-control-plane
we can delete this cluster with
> kind delete cluster -n local

# multi node setup in kind
we can create a multi node setup with yml
```yml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
```

> kind create cluster --config clusters.yml --name local

now if we check the list of running containers we will see 3 and the control plane will have an api address and a port where we can hit the kubernetes api
why? because the master node has an api server inside it

![[Pasted image 20241122021522.png]]

how to tell the master node to create a pod on a worker node? using the kubernetes API
but it requires authentication
To authenticate its stored in .kube/config
> cat ~/.kube/config

when we use kubectl it tries to find the content of ~/.kube/config
reads the auth creds from there and uses the kubernetes api to send commands to the master node's api server

when we create an aks cluster we they will give us a config file that we need to put in this directory ~./kube from the machine we want to send kubectl commands to the master node

# kubectl
## we can get all the nodes of the cluster with

> kubectl get nodes

if we want to see the api calls that are going out to the master node we can suffix any command with --v=8
> kubectl get nodes --v=8
![[Pasted image 20241122023122.png]]

## create a pod 

>`kubectl run <podName> --image=<imageName> --port=80`
>`kubectl run nginx --image=nginx --port=80`

what this creates looks something like this
![[Pasted image 20241122025603.png]]
its still not enough to expose it over the internet
to discover a pod is a whole different thing

this is one way to create a pod
## manifest
other way is create a manifest file
```yml
apiVersion: v1
kind: Pod
metadata:
	name: nginx
spec: 
	containers:
		- name: nginx
		  image: nginx
		  ports:
			  - containerPorts: 80
```

>`kubectl apply -f manifest.yml`

we can have multiple; pod configs in the same file and when we apply the file both would be applied
![[Pasted image 20241122182137.png]]
or we can create deployments
## get pods

>`kubectl get pods`
>`kubectl get pods -w`

this keeps watching we dont have to run the get pods command again and again
## logs of a pod

>`kubectl logs nginx`

## describe a pod

>`kubectl dscribe pod nginx`
![[Pasted image 20241122030435.png]]
## delete pod

>`kubectl delete pod <name>`

if i have multiple container running on the same pod they have the same ip address, because they share same network namespace, this enables to communicate via local host
If two containers in the same Pod try to bind to theÂ **same port**, a conflict will occur because both containers share the same network namespace. The second container will fail to start due to the port collision.

## deployment
higher level abstraction that manages a set of Pods and provides declarative updates to them
provides scaling rolling updates and rollback capabilities

 calculates the difference that needs to be applied by examining the current state and the end state that we want
 ![[Pasted image 20241122222107.png]]
a deployment technically creates a replicaset
replicaset is what mananges pods
## deployment example with 3 pods
deployment.yml
```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```
the matchlabels in the selector are what help identify the pods from this deployment
the template is then applied to the pods with the given labels

if after this i create e a pod with the label nginx the Deployment will delete it because it was told the final state should have 3 Pods with the label of nginx

the following assigns labels to the pods while 
```yaml
template:
	metadata: 
		labels:
			app: nginx
```

the selector is used to identify the pods belonging to this deployment

if we try to create another pod with the same label
the under lying replicaset will delete it, because it already has three pods being managed by it
>kubectl run nginx-pod --image=nginx --labels="app=nginx"

we can apply any of the yml manifests with, it doesnt have to be a pod manifest to apply it
> `kubectl apply -f <name>`

we can get our deployments with 
>kubectl get deployment

will show something like
![[Pasted image 20241126013050.png]]
now even if we delete a pod with kubectl delete the deployment (replicaset) will up another pod to match the deployment

technically a deployment is just a contract, it under the hood creates a replicaset and the replicaset makes sure that there are always n number of pods as mentioned in the contract

then why do we need deployments?
it provides features like rolling updates
it maintains a history of rollouts which can be inspected with

> kubectl rollout history deployment/nginx-deployment

we can update deployments configuration even after they are deployed and then roll them back

>kubectl set image deployment/nginx-deployment nginx=nginx:1.17

the `nginx=` in the above command comes from the container name that is defined in the manifest.yml
```yml
spec:
  containers:
  - name: nginx
	image: nginx:latest
```

we can undo the changes to the image the above command would have made to the deployment with
>kubectl rollout undo deployment/nginx-deployment

we can see the currently used image with 
>kubectl describe deployment nginx-deployment

we can get replicasets with 
>kubectl get replicaset

think of replica sets as state machines
it has the desired state and the current state and calculates the diff in an infinte loop to get to the desried state
the control-place has a controller for replicaset
the label selectors are used by the replicaset to identify and manage pods
![[Pasted image 20241126014658.png]]
clean up the clusters with 
>kubectl delete deployment nginx-deployment

it will delete the deployment the replicaset and the pods for us

## series of events when we apply a manifest

1. **Command Execution**:
- You execute the command on a machine with `kubectl` installed and configured to interact with your Kubernetes cluster.
2. **API Request**:
- `kubectl` sends a request to the Kubernetes API server to create a Deployment resource with the specified parameters.
3. **API Server Processing**:
- The API server receives the request, validates it, and then processes it. If the request is valid, the API server updates the desired state of the cluster stored in etcd. The desired state now includes the new Deployment resource.
4. **Deployment Controller Monitoring**:
- The Deployment controller, which is part of the `kube-controller-manager`, continuously watches the API server for changes to Deployments. It detects the new Deployment you created.
5. **ReplicaSet Creation**:
- The Deployment controller creates a ReplicaSet based on the Deployment's specification. The ReplicaSet is responsible for maintaining a stable set of replica Pods running at any given time.
6. **Pod Creation**:
- The ReplicaSet controller (another part of the `kube-controller-manager`) ensures that the desired number of Pods (in this case, 3) are created and running. It sends requests to the API server to create these Pods.
7. **Scheduler Assignment**:
- The Kubernetes scheduler watches for new Pods that are in the "Pending" state. It assigns these Pods to suitable nodes in the cluster based on available resources and scheduling policies.
8. **Node and Kubelet**:
- The kubelet on the selected nodes receives the Pod specifications from the API server. It then pulls the necessary container images (nginx in this case) and starts the containers.
## replica set manifest

```yml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

## rolling updates via deployments

lets say we first applied the below manifest
```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```
>kubectl apply -f manifest.yml

it will start three pods for us

if we change the image name to something incorrect like a typo with `nignx:latest`
and re apply the deployment manifest with the same name

kubernetes will first try to start one pod with this image, if it successful it will kill one of the old pods and try to create a new one again
K8s will do that in a loop untill the final state changes according to the new deployment, but incase it is not able to start a new pod with that image it wont delete the old pod
if we try to get all pods we will be able to see the status of one of the pods to be errored out
![[Pasted image 20241126132320.png]]

we can check more logs of the crashed deployment with
>kubectl logs -f nginx-deployment-54bd4799cb-964b7

in which case we can roll back our changes with
>kubectl rollout undo deployment/nginx-deployment

these are called rolling updates

## passing environment variables via the manifest file

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pg-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pg
  template:
    metadata:
      labels:
        app: pg
    spec:
      containers:
      - name: pg
        image: postgres:latest
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_PASSWORD
          value: "yourpassword"
```
we can simply apply this and this will supply the env variables
we can also set environment variables when creating pods with kubectl run command
>kubectl run example-pod --image=nginx --env="var1=val1" --env="var2=val2"

## Service (exposing pods)

since pods are ephemeral they die and come back up, and with new IP addresses, each pod has a unique IP address
its hard to keep track of them
that where services come in

a service is what exposes our pods to internet

we can see the IP address of our pods with
> kubectl get pods -owide

thse are private IPs and can be used by the pods to talk to each other
but not through the internet

![[Pasted image 20241127000045.png]]

an abstraction that logically define the pods and the set of policies by which to access them. 
allows us to expose application in pods as network services

Services use labels to select the pods that are being targeted

there are three types of services
1. **ClusterIP** - exposes the service on an internal IP of the cluster, this is the default service type. makes the pod accessible only within the cluster
2. **NodePort** - exposes the service on each Node's IP at a static port. the NodePort routes to a ClusterIP which is automatically created. 
  we can contact the underlying Pod from outside the cluster by requesting `<NodeIp>:<NodePort>`
  3. **LoadBalancer** - Exposes the service externally using a cloud providers load balancer. a loadbalancer routes to the NodePorts and ClusterIPs the load balancer routes to are created automatically

### manifest for a service

```yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30007  # This port can be any valid port within the NodePort range
  type: NodePort
```

this will make the pod with selector app: nginx accessible to the outside world on the IP address of the node it is on

but we dont know the IP of the node, we want to be able to use the IP address of the host machine which we do know
to be able to do that we want to map the nodePort of the node to a port of the host, try to keep them same

```yml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30007
    hostPort: 30007
- role: worker
- role: worker
```

if we start our cluster with this yaml it will map the port 30007 of our localhost to the port 30007 of the nodes
and then that port can be used in NodePort services

but this is only for KIND, this can be done via GUI on azure

now if we have three pods with label app: nginx, the NodePort service will automatically load balance over them

![[Pasted image 20241127005515.png]]
this is only doable in development with KIND
not recommended for production scenarios
this external port mapping on the kind node can be done on any of the nodes not necessarily required to be on the master node

in production we can simply deploy a loadbalancer that will provision an external IP address for us but on some cloud proovider
```yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service-lb
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80         # The port on the LoadBalancer
      targetPort: 80    # The port on the pods
  type: LoadBalancer   # This type triggers the provisioning of an external IP
```
![[Pasted image 20241127020221.png]]
