# kubectl
## we can get all the nodes of the cluster with

> kubectl get nodes

if we want to see the api calls that are going out to the master node we can suffix any command with --v=8
> kubectl get nodes --v=8
![[Pasted image 20241122023122.png]]

## create a pod 

>`kubectl run <podName> --image=<imageName> --port=80`
>`kubectl run nginx --image=nginx --port=80`

what this creates looks something like this
![[Pasted image 20241122025603.png]]
its still not enough to expose it over the internet
to discover a pod is a whole different thing

this is one way to create a pod
## manifest
other way is create a manifest file
```yml
apiVersion: v1
kind: Pod
metadata:
	name: nginx
spec: 
	containers:
		- name: nginx
		  image: nginx
		  ports:
			  - containerPorts: 80
```

>`kubectl apply -f manifest.yml`

we can have multiple; pod configs in the same file and when we apply the file both would be applied
![[Pasted image 20241122182137.png]]
or we can create deployments
## get pods

>`kubectl get pods`
>`kubectl get pods -w`

this keeps watching we dont have to run the get pods command again and again
## logs of a pod

>`kubectl logs nginx`

## describe a pod

>`kubectl dscribe pod nginx`
![[Pasted image 20241122030435.png]]
## delete pod

>`kubectl delete pod <name>`

if i have multiple container running on the same pod they have the same ip address, because they share same network namespace, this enables to communicate via local host
If two containers in the same Pod try to bind to theÂ **same port**, a conflict will occur because both containers share the same network namespace. The second container will fail to start due to the port collision.

## deployment
higher level abstraction that manages a set of Pods and provides declarative updates to them
provides scaling rolling updates and rollback capabilities

 calculates the difference that needs to be applied by examining the current state and the end state that we want
 ![[Pasted image 20241122222107.png]]
a deployment technically creates a replicaset
replicaset is what mananges pods
## deployment example with 3 pods
deployment.yml
```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```
the matchlabels in the selector are what help identify the pods from this deployment
the template is then applied to the pods with the given labels

if after this i create e a pod with the label nginx the Deployment will delete it because it was told the final state should have 3 Pods with the label of nginx

the following assigns labels to the pods while 
```yaml
template:
	metadata: 
		labels:
			app: nginx
```

the selector is used to identify the pods belonging to this deployment

if we try to create another pod with the same label
the under lying replicaset will delete it, because it already has three pods being managed by it
>kubectl run nginx-pod --image=nginx --labels="app=nginx"

we can apply any of the yml manifests with, it doesnt have to be a pod manifest to apply it
> `kubectl apply -f <name>`

we can get our deployments with 
>kubectl get deployment

will show something like
![[Pasted image 20241126013050.png]]
now even if we delete a pod with kubectl delete the deployment (replicaset) will up another pod to match the deployment

technically a deployment is just a contract, it under the hood creates a replicaset and the replicaset makes sure that there are always n number of pods as mentioned in the contract

then why do we need deployments?
it provides features like rolling updates
it maintains a history of rollouts which can be inspected with

> kubectl rollout history deployment/nginx-deployment

we can update deployments configuration even after they are deployed and then roll them back

>kubectl set image deployment/nginx-deployment nginx=nginx:1.17

the `nginx=` in the above command comes from the container name that is defined in the manifest.yml
```yml
spec:
  containers:
  - name: nginx
	image: nginx:latest
```

we can undo the changes to the image the above command would have made to the deployment with
>kubectl rollout undo deployment/nginx-deployment

we can see the currently used image with 
>kubectl describe deployment nginx-deployment

we can get replicasets with 
>kubectl get replicaset

think of replica sets as state machines
it has the desired state and the current state and calculates the diff in an infinte loop to get to the desried state
the control-place has a controller for replicaset
the label selectors are used by the replicaset to identify and manage pods
![[Pasted image 20241126014658.png]]
clean up the clusters with 
>kubectl delete deployment nginx-deployment

it will delete the deployment the replicaset and the pods for us

## series of events when we apply a manifest

1. **Command Execution**:
- You execute the command on a machine with `kubectl` installed and configured to interact with your Kubernetes cluster.
2. **API Request**:
- `kubectl` sends a request to the Kubernetes API server to create a Deployment resource with the specified parameters.
3. **API Server Processing**:
- The API server receives the request, validates it, and then processes it. If the request is valid, the API server updates the desired state of the cluster stored in etcd. The desired state now includes the new Deployment resource.
4. **Deployment Controller Monitoring**:
- The Deployment controller, which is part of the `kube-controller-manager`, continuously watches the API server for changes to Deployments. It detects the new Deployment you created.
5. **ReplicaSet Creation**:
- The Deployment controller creates a ReplicaSet based on the Deployment's specification. The ReplicaSet is responsible for maintaining a stable set of replica Pods running at any given time.
6. **Pod Creation**:
- The ReplicaSet controller (another part of the `kube-controller-manager`) ensures that the desired number of Pods (in this case, 3) are created and running. It sends requests to the API server to create these Pods.
7. **Scheduler Assignment**:
- The Kubernetes scheduler watches for new Pods that are in the "Pending" state. It assigns these Pods to suitable nodes in the cluster based on available resources and scheduling policies.
8. **Node and Kubelet**:
- The kubelet on the selected nodes receives the Pod specifications from the API server. It then pulls the necessary container images (nginx in this case) and starts the containers.
## replica set manifest

```yml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

## rolling updates via deployments

lets say we first applied the below manifest
```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```
>kubectl apply -f manifest.yml

it will start three pods for us

if we change the image name to something incorrect like a typo with `nignx:latest`
and re apply the deployment manifest with the same name

kubernetes will first try to start one pod with this image, if it successful it will kill one of the old pods and try to create a new one again
K8s will do that in a loop untill the final state changes according to the new deployment, but incase it is not able to start a new pod with that image it wont delete the old pod
if we try to get all pods we will be able to see the status of one of the pods to be errored out
![[Pasted image 20241126132320.png]]

we can check more logs of the crashed deployment with
>kubectl logs -f nginx-deployment-54bd4799cb-964b7

in which case we can roll back our changes with
>kubectl rollout undo deployment/nginx-deployment

these are called rolling updates

## passing environment variables via the manifest file

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pg-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pg
  template:
    metadata:
      labels:
        app: pg
    spec:
      containers:
      - name: pg
        image: postgres:latest
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_PASSWORD
          value: "yourpassword"
```
we can simply apply this and this will supply the env variables
we can also set environment variables when creating pods with kubectl run command
>kubectl run example-pod --image=nginx --env="var1=val1" --env="var2=val2"
### Namespaces

>kubectl get pods --all-namespaces
>kubectl get namespaces
>`kubectl get pods --namespace <name>`

we can create pods in a namespace by adding it to the metadata section of the manifest
```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```
when we make changes to a manifest and reapply it updates the old pods if they have the same name and are in the same namespace
but if we change the name or the namespace in the manifest kubernetes will create new pods for us as these two used to identify the deployment

we can set the context of the namesapce for the rest of my commands
if i set the context to backend then we can simply do kubectl get pods and tht will run for that namespace
>kubectl config set-context --current --namespace=server
>kubectl get pods 


## Service (exposing pods)

since pods are ephemeral they die and come back up, and with new IP addresses, each pod has a unique IP address
its hard to keep track of them
that where services come in

a service is what exposes our pods to internet

we can see the IP address of our pods with
> kubectl get pods -owide

thse are private IPs and can be used by the pods to talk to each other
but not through the internet

![[Pasted image 20241127000045.png]]

an abstraction that logically define the pods and the set of policies by which to access them. 
allows us to expose application in pods as network services

Services use labels to select the pods that are being targeted

there are three types of services
1. **ClusterIP** - exposes the service on an internal IP of the cluster, this is the default service type. makes the pod accessible only within the cluster
2. **NodePort** - exposes the service on each Node's IP at a static port. the NodePort routes to a ClusterIP which is automatically created. 
  we can contact the underlying Pod from outside the cluster by requesting `<NodeIp>:<NodePort>`
  3. **LoadBalancer** - Exposes the service externally using a cloud providers load balancer. a loadbalancer routes to the NodePorts and ClusterIPs the load balancer routes to are created automatically

### manifest for a service

```yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30007  # This port can be any valid port within the NodePort range
  type: NodePort
```

this will make the pod with selector app: nginx accessible to the outside world on the IP address of the node it is on

but we dont know the IP of the node, we want to be able to use the IP address of the host machine which we do know
to be able to do that we want to map the nodePort of the node to a port of the host, try to keep them same

```yml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30007
    hostPort: 30007
- role: worker
- role: worker
```

if we start our cluster with this yaml it will map the port 30007 of our localhost to the port 30007 of the nodes
and then that port can be used in NodePort services

but this is only for KIND, this can be done via GUI on azure

now if we have three pods with label app: nginx, the NodePort service will automatically load balance over them

![[Pasted image 20241127005515.png]]
this is only doable in development with KIND
not recommended for production scenarios
this external port mapping on the kind node can be done on any of the nodes not necessarily required to be on the master node

in production we can simply deploy a loadbalancer that will provision an external IP address for us but on some cloud proovider
```yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service-lb
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80         # The port on the LoadBalancer
      targetPort: 80    # The port on the pods
  type: LoadBalancer   # This type triggers the provisioning of an external IP
```
![[Pasted image 20241127020221.png]]
### drawbacks of services

if we have a microservice architecture it will require us to deploy a load balancer for each of the microservice
i will also require multiple certificates for each of the domain names mapped in the load balancers
each load balancer will require us to configure rate limiting in them individually
## Ingress

if we want to use ingresses we have to install the ingress controller
popular options are nginx-ingress-controller and haproxy-ingress-controller

ingress is an api object to manage external access to services in a cluster via http.
load balacing
ssl termination
name based virtual hosting
![[Pasted image 20241128081003.png]]
![[Pasted image 20241128102505.png]]
1. create deployment
2. create cluster ip service
3. create ingress - will create the load balancer for us

### installing ingress controller

nginx-ingress-controller.yml created by helm

helm helps manage these charts
>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
>helm repo update
>helm install nginx-ingress ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace

-1:16:46