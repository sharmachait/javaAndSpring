also has master slave architecture
master listens to the clients and spins up containers on workers
these managed containers, managed by the master instance is called a pod
we can have multiple containers in a single pod
whats the point of pods?
containers in the same pod can share teh file system
![[Pasted image 20241111234712.png]]

api server is the daemon that listens for kube commands
etcd is a distributed in memory cache
	multiple masters can share data in the etcd, responsible for implementing logs with in the cluster to ensure there are no conflicts
Scheduler
	responsible for distributing containers across multiple nodes
Controller
	controllers are the mind behind the orchestration, they monitor the pods/nodes and respond to the failure

- When an API server receives a request to create a pod, it first validates the request and stores the pod specification in etcd as the desired state.
- The kube-controller-manager continuously watches etcd for changes. When it detects a new pod specification, it checks the current state against the desired state and determines what actions are needed to reconcile them.
- The kube-scheduler also watches etcd for unscheduled pods. Its primary role is to determine which node a pod should be placed on based on various factors like resource requirements, node capacity, affinity/anti-affinity rules, and other scheduling constraints.
- Once the scheduler decides on a node, it updates the pod specification in etcd with the node information.
- The kubelet on the selected node watches etcd and sees the pod assignment. It then pulls the necessary container images and starts the pod on that node.

the controller manager manages
- deployment controller
- jobs controller
- replica set controller

Kubernetes features a container runtime to run the containers (containerd)
![[Pasted image 20241112000301.png]]

the container runtime is the allocated os resources where the containers run
kubelet is another daemon(agent) that keeps on running and spins up containers in the container runtime
kube-proxy is the reverse proxy manager, to redirect http requests to the correct containers

Containerd comes with a CLI tool called ctr
>ctr images pull docker.io/library/redis:alpine
>ctr run docker.io/library/redis:alpine redis

very limited better to use the nerdctl tool, it is almost like docker 
provides access to features not yet implemented in docker
>nerdctl run --name redis -p 80:80 -d redis:alpine

these two tools are containerd specific
crictl is a cli tool which is runtime agnostic as long as the runtime is CRI compatible
used primarily only for inspection and debugging
# KIND 

to create kubernetes cluster on your local machine
> curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.18.0/kind-linux-amd64
> chmod +x ./kind
> sudo mv ./kind /usr/local/bin/kind
> kind--version
> kind create cluster --name local

with this we will be able to create a master node on our machine, basically a container named local-control-plane
we can delete this cluster with
> kind delete cluster -n local

# multi node setup in kind
we can create a multi node setup with yml
```yml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
```

> kind create cluster --config clusters.yml --name local

now if we check the list of running containers we will see 3 and the control plane will have an api address and a port where we can hit the kubernetes api
why? because the master node has an api server inside it

![[Pasted image 20241122021522.png]]

how to tell the master node to create a pod on a worker node? using the kubernetes API
but it requires authentication
To authenticate its stored in .kube/config
> cat ~/.kube/config

when we use kubectl it tries to find the content of ~/.kube/config
reads the auth creds from there and uses the kubernetes api to send commands to the master node's api server

when we create an aks cluster we they will give us a config file that we need to put in this directory ~./kube from the machine we want to send kubectl commands to the master node

> kubectl cluster-info
> kubectl get nodes