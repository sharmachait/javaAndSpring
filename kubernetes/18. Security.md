first line of defense - controlling access to the api server

we can control who can have access with
1. username and passwords
2. username and tokens
3. certificates
4. external auth providers
5. service accounts
we can control what access a user has with
1. RBAC
2. ABAC attribute based
3. Node based authorization
4. webhook mode - used to use third party authorization managers
5. alwaysAllow
6. allwaysDeny

the mode is set in the kube api server startup command as an option, by default is alwaysAllow

we can use multiple modes and can be passed in as a comma separated list to the kube api server

when multiple are set they are used in that order ( in an OR operation) for every request, kinda like an auth pipeline 

all communication between the internal modules of k8s is TLS encrypted

Bots and automation agents can be authorized with service accounts
Users (admins/developers) can be authorized at the kube api server level using
1. static passwords file, CSV, we can save the data in a CSV file 
2. static token file, CSV
3. certificates
4. third party auth providers
### authentication setup
if we save the users and passwords in a CSV file we have it define it in the options for the **kube-apiserver.service** and restart the the server
or we can modify the kube-apiserver pod manifest file and add it as an option to the command
**--basic-auth-file=user-details.csv**
these must be passed as basic credentials using curl to hit kubernetes api
the user-details.csv file has password user name the id and groups
```csv
password123,user1,u0001,group1
password123,user12,u0002,group1
```
everything is same for the static token file just instead of password it stores the token and can be passed in with
**--token-auth-file=user-token.details.csv**
this token needs to be passed in the Authorization header as a Bearer token when communicating with the kube-api-server

obviously these are not recommended approaches

when using curl we need to pass creds, we can instead start a kubectl proxy session that will proxy all our api calls with creds to the right kubernetes cluster, we instead need to hit this proxy server 
> kubectl proxy

this starts a server at localhost:8001, we can do http calls against this server and they will be proxied with credentials to the right ip address and port

> `curl http:localhost:8001/api/v1/pod`


### authenticating with certificates
we can provide options for the cert in kubectl commands to authenticate our api requests
> kubectl get pods --server kube-cert-server:6443 --client-key admin.key --client-certificate admin.crt --certificate-authority ca.crt
### kube config

the certificate options and infromation in the above command can be instead written into a kube-config file that will be used for each command
**$HOME/.kube/config**

the kube config file has information about clusters contexts and user accounts

we can switch between different clusters and different user accounts and different contexts

contexts are used to form join relationships between clusters and user accounts

we never explicitly switch clusters, we switch contexts

a context => cluster + user-account + namespace

the server information is stored in the clusters section
the user keys and certs are stored in the users section of the kube config

```yml
apiVersion: v1
kind: Config
current-context: dev-admin@devcluster
clusters:
- name: dev-cluster
  cluster: 
    certificate-authority: etc/kubernetes/pki/ca.cert
    server: https://dev-cluster:6443
contexts:
- name: dev-admin@dev-cluster
  context:
    cluster: dev-cluster
    user: dev-admin
    namespace: dev
users:
- name: dev-admin
  user:
    client-certificate: etc/kubernetes/pki/users/admin.crt
    client-key: etc/kubernetes/pki/users/admin.key
```

the current context is used as the context by default

we can view the config with
>kubectl config view

we can change which config file should be used by kubectl with
>kubectl config view --kubeconfig=custom-config

to change the context
>kubectl config use-context prod-user@production

to change the context
>k config set-context --current --namespace=default 

this will update the kube-config  file to reflect the currently selected context
the namespace being used is defined in the namespace section of the context

to make a different file be the default kube config we can change the env variable KUBECONFIG to the path of the new kube config from the bashrc

### API groups

can be found out with
>kubectl api-resources | grep jobs

and for a particular resource type
>kubectl explain job
>kubectl explain job.spec.template 

and we can keep going like so

the api endpoints that the kube api server serves are grouped together 
1. /metrics
2. /healthz
3. /version
4. /api - core
	1. namespaces
	2. pods
	3. nodes
	4. pv
	5. pvc
	6. cm
	7. secrets
	8. services
5. /apis - named
	1. apps
		1. deployments
		2. replicasets
		3. statefulsets
	2. extensions
	3. networking.k8s.io
		1. network policies
	4. storage.k8s.io
	5. authentication.k8s.io
	6. certificates.k8s.io
6. /logs
##### API versions

1. /v1alpha1 - 
	1. is the version for any new api when its first developed and merged into the kubernetes code base, and is released for the very first time with a k8s version
	2. not enabled by default, isnt completely tested and reliable, may have bugs, may be dropped later
	3. can be enabled by adding to the kube api server config at `/etc/kubernetes/manifests/kube-apiserver.yaml` as **--runtime-config=batch/v2alpha1** in a comma separated format
2. /v1beta1 - 
	1. enabled by default, is e2e tested, may have minor bugs, is accepted to be productionized 
3. GA - general availability stable version
	1. will not be deprecated for many future releases, is reliable and completely tested
##### preferred versions

when we use imperative commands, preferred versions are the versions of the apis that will be used by kubectl
we can check the the preferred version in the response to `curl <api>`

> kuebctl proxy 8002
> curl localhost:8002/apis/api-group

or 

> kubectl api-versions | grep authorization.k8s.io
###### storage version

the data in etcd store needs to be consistent, so what ever version is used to create a resource, that version is selected to be the storage version
only one version can serve as the storage version for any resource
when we save an object k8s converts it to the storage version
when retrieving object k8s converts storage version to the requested version

only way to check the storage version is to query the etcd database it self

> etcdctl --endpoints-https://ip:port --cacert=path --cert=path --key=path get "registry/deployment/default/blue" --print-value-only

#### Api deprecation

why do we need multiple versions, governed by api deprecation policy
if an alpha version is very buggy the OS project working on it cant just simply drop it because of api deprecation policy rules

1. API elements may only be removed by incrementing the version of the API group, by going to v1alpha2, while the resource continues to exist in v1alpha1
	1. but if that happens any one using it will need to update their manifests, therefore it must support everything else being done by the previous version as well
2. API objects must be able to round trip between API versions in a given release without information loss with the exception of some resources that may be dropped in some release
	1. the conversion to any version for storage into etcd to conform to the storage version must not cause any information loss
	2. if v1alpha1 has a field in the manifest file, the future versions must necessarily have that field, they can not drop it from their manifest, can only add new
	3. and if we were to convert back from new to old version, the old version must have the same field but with empty value so that data is not lost
3. if our new version is also alpha we are not required to keep the older version of the api, same applies to beta versions but the older one must be supported for 9 months or 3 releases and for the stable versions the older one must be supported for 12 months or 3 releases, the alpha versions can go bye bye. the older versions which need to be supported for 9/12 months are called Deprecated
4. if we upgrade to beta from alpha, the alpha doesnt need to be supported
5. the preferred/storage version of an API can not be changed until after the first release where the preferred/storage version was deprecated in the release after that we can change the preferred/storage version
6. an api version can not be deprecated until a new api version at least as stable as that one is released, beta can deprecate alpha but alpha can not deprecate beta or stable

#### kubectl convert
Manifest can be converted to one api version from another with 
> kubectl convert -f oldfile --output-version new-api

this will change the api version in the manifest file
### authorization with RBAC 

authorization for a kubelet is handled by a Node Authorizer and is distinguished from a user by its certificate having **system:node:node01** group
this group has special privileges required for cluster management

for users we use Attribute based access
we can create a set of policy by creating a policy file in json format
```json
{
	"kind":"Policy",
	"spec":{
		"user":"dev-user",
		"namespace": "*",
		"resource": "pods",
		"apiGroup": "*"
	}
}
```

this policy file needs to be passed into the kube api server

this is difficult to manage 

RBAC is better

create a role and bind users to the roles

#### creating a role and role binding

to find your own username find it in
> kubectl config view

```yml
apiVersion: rbac.auhtorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "get", "create", "update", "delete"]
- apiGroups: [""]
  resources: ["configMap"]
  verbs: ["create"]
```

```yml
apiVersion: rbac.auhtorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developer-user-binding
subjecs:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
```

these make the user have access to pods and configMaps in the default namespace because no namespace was defined in the manifest files
must be defined in the metadata if we want to limit to some other namespace

> kubectl get roles
> kubectl get rolebindings
> kubectl describe role user-role
> kubectl describe rolebinding dev-user-user-role-binding


to check if i have access to do things
> kubectl auth can-i create deployments
> kubectl auth can-i delete nodes

an admin can even impersonate other users
> kubectl auth can-i create deployments --as dev-user
> kubectl auth can-i create deployments --as dev-user --namespace prod


we can restrict a users access to a particular set of pods, only one pod if required, by using its name
```yml
apiVersion: rbac.auhtorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "get", "create", "update", "delete"]
  resourceNames: ["ws","http"]
- apiGroups: [""]
  resources: ["configMap"]
  verbs: ["create"]
  resourceNames: ["ws","http"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["create"]
```
the subjects of role bindings can be service accounts as well

```yml
# ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ckad23-sa-aecs
  namespace: ckad23-nssa-aecs
---
# Role
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: wide-access-aecs
  namespace: ckad23-nssa-aecs
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["get", "list"]
---
# RoleBinding 
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: wide-access-rb-aecs
  namespace: ckad23-nssa-aecs
subjects:
- kind: ServiceAccount
  name: ckad23-sa-aecs
  namespace: ckad23-nssa-aecs
roleRef:
  kind: Role
  name: wide-access-aecs
  apiGroup: rbac.authorization.k8s.io
```
or even user groups
### Cluster roles and cluster role bindings

normal user roles are bound to namespaces

resources have two scopes
1. cluster scope- nodes, PV, namespaces
2. namespace scope -  

there fore roles need to be in these two scopes as well

```yml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-admin-role
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["list", "get", "create", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-binding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-admin-role
  apiGroup: rbac.authorization.k8s.10
```

cluster roles are canonically used for cluster scoped resources, but that is not a hard rule, we can create them for namespace scoped resources as well
in that case the user will have access to resources across namespace

in this cluster-admin is a string and not an object, and any user that authenticates with this identity has this role, this identity can be part of token claims

we can see all poissible resources for which we can create roles with
> kubectl api-resources

we can check the api version for a resource with
> kubectl api-resource | grep persistentvolume

```yml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io
```