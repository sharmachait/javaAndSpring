first line of defense - controlling access to the api server

we can control who can have access with
1. username and passwords
2. username and tokens
3. certificates
4. external auth providers
5. service accounts
we can control what access a user has with
1. RBAC
2. ABAC attribute based
3. Node based authorization
4. webhook mode - used to use third party authorization managers
5. alwaysAllow
6. allwaysDeny

the mode is set in the kube api server startup command as an option, by default is alwaysAllow

we can use multiple modes and can be passed in as a comma separated list to the kube api server

when multiple are set they are used in that order ( in an OR operation) for every request kinda like an auth pipeline 

all communication between the internal modules of k8s is TLS encrypted

Bots and automation agents can be authorized with service accounts
Users (admins/developers) can be authorized at the kube api server level using
1. static passwords file, CSV, we can save the data in a CSV file 
2. static token file, CSV
3. certificates
4. third party auth providers

### authentication setup
if we save the users and passwords in a CSV file we have it define it in the options for the **kube-apiserver.service** and restart the the server
or we can modify the kube-apiserver pod manifest file and add it as an option to the command
**--basic-auth-file=user-details.csv**
these must be passed as basic credentials using curl to hit kubernetes api
the user-details.csv file has password user name the id and groups
```csv
password123,user1,u0001,group1
password123,user12,u0002,group1
```
everything is same for the static token file just instead of password it stores the token and can be passed in with
**--token-auth-file=user-token.details.csv**
this token needs to be passed in the Authorization header as a Bearer token when communicating with the kube-api-server

obviously these are not recommended approaches

when using curl we need to pass creds, we can instead start a kubectl proxy session that will proxy all our api calls with creds to the right kubernetes cluster, we instead need to hit this proxy server 
> kubectl proxy

this starts a server at localhost:8001, we can do http calls against this server and they will be proxied with credentials to the right ip address and port

> `curl http:localhost:8001/api/v1/pod`


### authenticating with certificates
we can provide options for the cert in kubectl commands to authenticate our api requests
> kubectl get pods --server kube-cert-server:6443 --client-key admin.key --client-certificate admin.crt --certificate-authority ca.crt
### kube config

the certificate options and infromation in the above command can be instead written into a kube-config file that will be used for each command
**$HOME/.kube/config**

the kube config file has information about clusters contexts and user accounts

we can switch between different clusters and different user accounts and different contexts

contexts are used to form join relationships between clusters and user accounts

we never explicitly switch clusters, we switch contexts

a context => cluster + user-account

the server information is stored in the clusters section
the user keys and certs are stored in the users section of the kube config

```yml
apiVersion: v1
kind: Config
current-context: dev-admin@devcluster
clusters:
- name: dev-cluster
  cluster: 
    certificate-authority: etc/kubernetes/pki/ca.cert
    server: https://dev-cluster:6443
contexts:
- name: dev-admin@dev-cluster
  context:
    cluster: dev-cluster
    user: dev-admin
    namespace: dev
users:
- name: dev-admin
  user:
    client-certificate: etc/kubernetes/pki/users/admin.crt
    client-key: etc/kubernetes/pki/users/admin.key
```

the current context is used as the context by default

we can view the config with
>kubectl config view

we cna change which config file should be used by kubectl with
>kubectl config view --kubeconfig=custom-config

to change the context
>kubectl config use-context prod-user@production

this will update the kube-config  file to reflect the currently selected context

the namespace being used is defined in the namespace section of the context

we can use the kubectl command to change the namespace

>kubectl config set-context --current --namespace=server

in the above config example instead of path we can directly provide the data of the certificate as well but in base64 encoded format

to make a different file be the default kube config we can change the env variable KUBECONFIG to the path of the new kube config from the bashrc

### API groups

the api endpoints that the kube api server serves are grouped together 
1. /metrics
2. /healthz
3. /version
4. /api - core
	1. namespaces
	2. pods
	3. nodes
	4. pv
	5. pvc
	6. cm
	7. secrets
	8. services
5. /apis - named
	1. apps
		1. deployments
		2. replicasets
		3. statefulsets
	2. extensions
	3. networking.k8s.io
		1. network policies
	4. storage.k8s.io
	5. authentication.k8s.io
	6. certificates.k8s.io
6. /logs

### authorization with RBAC 

authorization for a kubelet is handled by a Node Authorizer and is distringuished from a user by its certificate having **system:node:node01** group
this group has special privileges required for cluster management

for users we use Attribute based access
we can create a set of policy by creating a policy file in json format
```json
{
	"kind":"Policy",
	"spec":{
		"user":"dev-user",
		"namespace": "*",
		"resource": "pods",
		"apiGroup": "*"
	}
}
```

this policy file needs to be passed into the kube api server

this is difficult to manage 

RBAC is better

create a role and bind users to the roles

#### creating a role and role binding
```yml
apiVersion: rbac.auhtorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "get", "create", "update", "delete"]
- apiGroups: [""]
  resources: ["configMap"]
  verbs: ["create"]
```

```yml
apiVersion: rbac.auhtorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developer-user-binding
subjecs:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
```

these make the user have access to pods and configMaps in the default namespace because no namespace was defined in the manifest files
must be defined in the metadata if we want to limit to some other namespace

> kubectl get roles
> kubectl get rolebindings
> kubectl describe role user-role
> kubectl describe rolebinding dev-user-user-role-binding


to check if i have access to do things
> kubectl auth can-i create deployments
> kubectl auth can-i delete nodes

an admin can even impersonate other users
> kubectl auth can-i create deployments --as dev-user
> kubectl auth can-i create deployments --as dev-user --namespace prod


we can restrict a users access to a particular set of pods, only one pod if required, by using its name
```yml
apiVersion: rbac.auhtorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "get", "create", "update", "delete"]
  resourceNames: ["ws","http"]
- apiGroups: [""]
  resources: ["configMap"]
  verbs: ["create"]
  resourceNames: ["ws","http"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["create"]
```

### Cluster roles and cluster role bindings

normal user roles are bound to namespaces

resources have two scopes
1. cluster scope- nodes, PV, namespaces
2. namespace scope -  

there fore roles need to be in these two scopes as well

```yml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-admin-role
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["list", "get", "create", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-binding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-admin-role
  apiGroup: rbac.authorization.k8s.10
```

cluster roles are canonically used for cluster scoped resources, but that is not a hard rule, we can create them for namespace scoped resources as well
in that case the user will have access to resources across namespace

in this cluster-admin is a string and not an object, and any user that authenticates with this identity has this role, this identity can be part of token claims

we can see all poissible resources for which we can create roles with
> kubectl api-resources

we can check the api version for a resource with
> kubectl api-resource | grep persistentvolume

```yml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io
```


## admission controllers
all these roles are only restrictions to the k8s api

but we may want to be able to restrict k8s itself to use images from an internal registry only
or to restrict pods from being run as root users
or that metadata for each resource always contains labels

such restrictions can not be achieved with roles

admission controllers help us achieve control over such details

#### list of admission controllers

1. AlwaysPullImages - to enforce that local images are not used and a new image is always pulled from the registry
2. DefaultStorageClass - automatically adds the default storage class to PVCs as they are created
3. EventRateLimit - to rate limit the number of requests the api server will handle
4. NamespaceExists - restricts requests to namespaces that dont exist, enabled by default
5. NamespaceAutoProvision - when enabled creates a namespace if doesnt exist for every api request

to see the list of admission controllers enabled by default

> kube-apiserver -h | grep enable-admission-plugins

this may need to be run in the control plane node
can be achieved with
> kubectl exec kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep enable-admission-plugins

or if the kube-apiserver is running as a pod

> ps -ef | grep kube-apiserver | grep admission-plugins

we can exec into a pod with
>`kubectl exec -it <pod-name> -- /bin/bash`

if a pod has multiple containers it will ask for container name
> `kubectl exec -it <pod-name> -c <container-name> -- /bin/bash`

to enable or disable a new admission controller it can be added to the pod manifest of the kube-apiserver

```yml
apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
  labels:
    component: kube-apiserver
    tier: control-plane
spec:
  containers:
  - name: kube-apiserver
    image: k8s.gcr.io/kube-apiserver:v1.28.0  # Replace with your Kubernetes version
    command:
    - kube-apiserver
    - --advertise-address=192.168.0.1  # Replace with your control plane node IP
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision
    - --disable-admission-plugins=DefaultStorageClass
```


NamespaceExists and NamespaceAutoprovision controllers are deprecated and NamespaceLifecycle controller is recommended, it also makes sure that the dafault and k8s's namespaces can not be deleted


the kube api server can be edited by editing its manifest at **/etc/kubernetes/manifests/kube-apiserver.yaml**
k8s will automatically reconfigure the kube-api-server accordingly

