### Rollback, upgrade mechanism

we we create a deployment, i triggers a rollout
a new rollout creates a new deployment revision
for each upgrade a new rollout and a new revision is created

this allows us to rollback to a previous version

> kubectl rollout status deployment-name

to see the revisions and history
>kubectl rollout history deployment-name

we can annotate and label pods with imperative command as well
but better to use the manifest file

when we make changes and upgrade our application deployment creates another replicaset for us

to undo a rollout and go back to a previous revision
>kubectl rollout undo deployment-name 

this reverses the process of upgrade where pods from new replicaest are deleted and brought up in the old replicaset

To rollback to specific revision we will use the `--to-revision` flag.  
>kubectl rollout undo deployment nginx --to-revision=1

With `--to-revision=1`, it will be rolled back with the first image we used to create a deployment as we can see in the `rollout history` output.

we can record the change cause for an edit/apply of manifest with the `--record` flag

to see how many pods are allowed to go down in an upgrade of a deployment
check the rolloutstrategy in its manifest

### Strategies

###### Rolling updates and Recreate
when we create a deployment by default the rolling update strategy is used

its defined in **.spec.strategy.type=RollingUpdate**
we can specify how many pods from the old replicaset can be killed in  **.spec.strategy.rollingUpdate.maxUnaliveable=25%**, can be a number as well
we can specify the max number of extra pods from the replicaset in **.spec.strategy.rollingUpdate.maxSurge=25%** can be a number as well

we can change the strategy by setting **.spec.strategy.type=Recreate**

###### Blue Green
cant be directly implemented
the new version in its entirety is deployed alongside the old version, with the traffic still routing to the old version, then the routing is changed and the old version is brought down

to implement
1. create a new deployment
2. edit the service, the selector matchlabels for the pods
3. delete the old deployment 

###### Canary deployment
cant be directly implemented
the entire new application is deployed and traffic is redirected to the new pods incrementally 

to implement
1. create a new deployment
2. add a common label to the pods of both the deployments that can be shared and used by the service to direct traffic to both the deployments
3. to reduce the amount of traffic going to the new deployment we can simply reduce the number of pods in the new deployment, 
4. if the number of pods in both the deployment is same the traffic will eb distributed across them equally
5. slowly increase the number of pods in the new deployment
6. after all wanted pods are deployed, delete the old deployment